<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>networking on GUI Free Life</title><link>http://guifreelife.com/tags/networking/</link><description>Recent content in networking on GUI Free Life</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 15 Feb 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://guifreelife.com/tags/networking/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenShift 4 on OpenStack Networking and Installation</title><link>http://guifreelife.com/blog/2020/02/15/OpenShift-4-on-OpenStack-Networking-and-Installation/</link><pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate><guid>http://guifreelife.com/blog/2020/02/15/OpenShift-4-on-OpenStack-Networking-and-Installation/</guid><description>OpenShift Containter Platform 4 is much more like Tectonic than OpenShift 3. Particularly when it comes to installation and node management. Rather then building machines and running an Ansible playbook to configure them you now have the option of setting a fewer paramters in an install config running an installer to build and configure the cluster from scratch.
I would like to illustrate how the basics of the networking might look when installing OpenShift on OpenStack.</description></item><item><title>Creating OpenStack Provider Network for Use by a Single Project</title><link>http://guifreelife.com/blog/2018/10/30/OpenStack-Private-Provider-Network/</link><pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate><guid>http://guifreelife.com/blog/2018/10/30/OpenStack-Private-Provider-Network/</guid><description>OpenStack supports &amp;ldquo;provider&amp;rdquo; networks, which are networks that pre-exist in your physical infrastructure and are &amp;ldquo;provided&amp;rdquo; to the cloud users rather than created by the user. Only an admin is permitted to create a provider network.
A prequisite is the provider network must be plumbed to the external bridge on your controller and nova nodes.
Here is an Ansible playbook to create a project, place a unshared provider network and subnet in that project.</description></item><item><title>Load balancing of OpenShift HA Routers Mind the GARP</title><link>http://guifreelife.com/blog/2018/02/16/OpenShift-Router-LoadBalancing/</link><pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate><guid>http://guifreelife.com/blog/2018/02/16/OpenShift-Router-LoadBalancing/</guid><description>OpenShift HA Routing uses haproxy application routers to get traffic into the cluster. These application routers are made redundant by running ipfailover (keepalived) pods to maintain a set of Virtual IPs on each infrastructure node where the application routers run. These VIPs are then referenced by round robin DNS records to enable a measure of load balancing.
OK, so now you are load balancing at the network layer, but what about the link layer?</description></item><item><title>OpenShift High Availability - Routing</title><link>http://guifreelife.com/blog/2016/03/01/OpenShift-3-HA-Routing/</link><pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate><guid>http://guifreelife.com/blog/2016/03/01/OpenShift-3-HA-Routing/</guid><description>Highly availabile containers in OpenShift are baked into the cake thanks to replication controllers and service load balancing, but there are plenty of other single points of failure. Here is how to eliminate many of those.
Single Points of Failure The components of OpenShift include:
Master controller manager server and API endpoint Etcd configuration and state storage Docker Registry Router haproxy This post is mostly about adding high availability to the routing layer.</description></item><item><title>Notes on SNMP MIBs OIDs and Grey Whiskers</title><link>http://guifreelife.com/blog/2015/09/08/SNMP-OIDs/</link><pubDate>Tue, 08 Sep 2015 00:00:00 +0000</pubDate><guid>http://guifreelife.com/blog/2015/09/08/SNMP-OIDs/</guid><description>With as many grey whiskers as I have, you would think I could grok SNMP in my sleep by now. Unfortunately, everytime I have to deal with it I get frustrated, wonder where the hell my notes from last time are, and start cursing. From now on, here are my notes!
I&amp;rsquo;m typically using Zabbix to poll SNMP OIDs and place the MIBs on the Zabbix server or the Zabbix proxy responsible for SNMP in /usr/share/snmp/mibs.</description></item></channel></rss>