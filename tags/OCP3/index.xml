<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OCP3 on dwnwrd Universal</title><link>https://dwnwrd.github.io/tags/OCP3/</link><description>Recent content in OCP3 on dwnwrd Universal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 09 May 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://dwnwrd.github.io/tags/OCP3/index.xml" rel="self" type="application/rss+xml"/><item><title>Playbook to replace bootstrap.kubeconfig and node certificates on OpenShift 3.10 3.11</title><link>https://dwnwrd.github.io/blog/2019/05/09/Replace-Bootstrap-Kubeconfig/</link><pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2019/05/09/Replace-Bootstrap-Kubeconfig/</guid><description>If you are a serial upgrader like me, you may have found that at one point during your 3.10.xx patching (say 3.10.119) you hit this error during the data plane upgrade:
TASK [openshift_node : Approve the node] ************************************************************ task path: /usr/share/ansible/openshift-ansible/roles/openshift_node/tasks/upgrade/restart.yml:49 Using module file /usr/share/ansible/openshift-ansible/roles/lib_openshift/library/oc_csr_approve.py ... FAILED - RETRYING: Approve the node (30 retries left).Result was: { &amp;#34;all_subjects_found&amp;#34;: [], &amp;#34;attempts&amp;#34;: 1, &amp;#34;changed&amp;#34;: false, &amp;#34;client_approve_results&amp;#34;: [], &amp;#34;client_csrs&amp;#34;: {}, &amp;#34;failed&amp;#34;: true, &amp;#34;invocation&amp;#34;: { &amp;#34;module_args&amp;#34;: { &amp;#34;node_list&amp;#34;: [ &amp;#34;ose-test-node-01.</description></item><item><title>Downgrade Etcd 3.3.11 to 3.2.22 for OpenShift Compatibility</title><link>https://dwnwrd.github.io/blog/2019/02/19/Downgrade-Etcd-for-OpenShift-Compatibility/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2019/02/19/Downgrade-Etcd-for-OpenShift-Compatibility/</guid><description>While I was working on migrating etcd to my master nodes I was bitten by an incompatible etcd v3.3.11 RPM made available via RHEL Server Extras repo. Before I got to my last master the RPM was no longer available, and the scaleup playbook failed. I became aware that 3.3.11 is not compatible and should not have been made available.
Unfortunately all members of my etcd cluster were already upgraded and the fix is to take down the cluster, downgrade etcd, and restore from snapshot.</description></item><item><title>Migration of Etcd to Masters for OpenShift 3.9 to 3.10 Upgrade</title><link>https://dwnwrd.github.io/blog/2019/02/08/Migration-of-Etcd-to-Masters-for-OpenShift-3.9-Upgrade-to-3.10/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2019/02/08/Migration-of-Etcd-to-Masters-for-OpenShift-3.9-Upgrade-to-3.10/</guid><description>As of OpenShift Container Platform 3.10 etcd is expected to run in static pods on the master nodes in the control plane. You may have a deployed an HA cluster with dedicated etcd nodes managed with systemd. How do you migrate the this new architecture?
Assumptions:
You are running OCP 3.9 You have multiple Master nodes You have dedicated Etcd nodes You are running RHEL, not Atomic nodes Outline:</description></item><item><title>OpenShift 3.6 Upgrade Metrics Fails Missing heapster-certs Secret</title><link>https://dwnwrd.github.io/blog/2017/10/13/OpenShift-3.6-Upgrade-Metrics-Fails-Missing-heapster-certs-Secret/</link><pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2017/10/13/OpenShift-3.6-Upgrade-Metrics-Fails-Missing-heapster-certs-Secret/</guid><description>After your upgrade to OpenShift v3.6 did the deployment of cluster metrics wind up with empty graphs? Check if the heapster pod failed to start due to a missing secret called heapster-certs in the openshift-infra namespace.
Problem Heapster pod is failing to start
$ oc get pods NAME READY STATUS RESTARTS AGE hawkular-cassandra-1-l1f3s 1/1 Running 0 9m hawkular-metrics-rdl07 1/1 Running 0 9m heapster-cfpcj 0/1 ContainerCreating 0 3m Check what volumes it is attempting to mount</description></item><item><title>Automated Pruning of OpenShift Artifacts; Builds, Deploys, Images</title><link>https://dwnwrd.github.io/blog/2017/03/22/Automated-OpenShift-Artifact-Pruning/</link><pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2017/03/22/Automated-OpenShift-Artifact-Pruning/</guid><description>After running openshift for a while I discovered that letting builds pile up to around to around 1,200 led to what was essentially a deadlock in the scheduling of new builds. New builds were stuck in a New, waiting state indefinitely.
This was fixed as of OCP 3.4.1, but it caused me to get more pro-active in the pruning of artifacts within OpenShift.
I threw together a script and a playbook to deploy it.</description></item><item><title>Configuring OpenShift with Multiple Sharded Routers</title><link>https://dwnwrd.github.io/blog/2017/01/29/OpenShift-Multiple-Sharded-Routers/</link><pubDate>Sun, 29 Jan 2017 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2017/01/29/OpenShift-Multiple-Sharded-Routers/</guid><description>I needed to host a service that would be consumed by a closed client that insists on speaking HTTPS on port 50,000. To solve this, I added a 2nd router deployment and used the OpenShift router sharding feature to selectively enable routes on the 2nd router by way of selectors.
To summarize:
Existing HA router:
HTTP 80 HTTPS 443 Haproxy Stats 1,936 Added HA router:
HTTP 49,999 HTTPS 50,000 Haproxy Stats 51,936 How To Open infra node firewalls Open firewall on infra nodes where router will run to allow new http and https port iptables -A OS_FIREWALL_ALLOW -m tcp -p tcp --dport 49999 -j ACCEPT iptables -A OS_FIREWALL_ALLOW -m tcp -p tcp --dport 50000 -j ACCEPT This can also be done with Ansible and the os_firewall role in your playbook.</description></item><item><title>OpenShift Cluster Metrics and Cassandra Troubleshooting</title><link>https://dwnwrd.github.io/blog/2016/11/14/OpenShift-Cluster-Metrics-and-Cassandra-Troubleshooting/</link><pubDate>Mon, 14 Nov 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/11/14/OpenShift-Cluster-Metrics-and-Cassandra-Troubleshooting/</guid><description>OpenShift gathers cluster metrics such as CPU, memory, and network bandwidth per pod which can assist in troubleshooting and capacity planning. The metrics are also used to support horizontal pod autoscaling, which makes the metrics service not just helpful, but critical to operation.
Missing Liveness Probes There are 3 major components in the metrics collection process. Heapster gathers stats from Docker and feeds them to Hawkular Metrics to tuck away for safe keeping in Cassandra.</description></item><item><title>How to List Tags On Redhat Registry Images</title><link>https://dwnwrd.github.io/blog/2016/07/11/List-Tags-On-Redhat-Registry-Images/</link><pubDate>Mon, 11 Jul 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/07/11/List-Tags-On-Redhat-Registry-Images/</guid><description>Ever gone to RedHat&amp;rsquo;s container registry to search for an image and been left wondering what versions exist? Ever been frustrated by the inconsistent tag format? Is there a v or is there not a v? Me too.
Docker Hub has progressed to v2, while the RedHat registry is still v1 at the moment. As long as you use the right syntax, you can use curl to query the registry API and list the tags like this:</description></item><item><title>Deploy Hawkular Metrics in CDK 2.1 OpenShift 3.2</title><link>https://dwnwrd.github.io/blog/2016/06/16/Deploy-Hawkular-Metrics-in-CDK-2.0-OpenShift-3.1/</link><pubDate>Thu, 16 Jun 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/06/16/Deploy-Hawkular-Metrics-in-CDK-2.0-OpenShift-3.1/</guid><description>Update! I failed with CDK 2.0, but CDK 2.1 works with some fiddling.
In my last post I installed Red Hat Container Developer Kit to deploy OpenShift Enterprise using Vagrant. But now I want to add Hawkular Metrics to that deployment.
Deploy Metrics Refer to the docs for deploying metrics in OSE.
Login to the vagrant CDK VM before continuing
$ cd ~/cdk/components/rhel/rhel-ose/ $ vagrant ssh $ oc login Authentication required for https://127.</description></item><item><title>Getting Started With RedHat Container Development Kit</title><link>https://dwnwrd.github.io/blog/2016/06/16/Getting-Started-With-RedHat-Container-Development-Kit/</link><pubDate>Thu, 16 Jun 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/06/16/Getting-Started-With-RedHat-Container-Development-Kit/</guid><description>The RedHat Container Developer Kit allows you to deploy OpenShift on your laptop for easier testing and development. Here is how to deploy it.
Register as a RedHat Developer Obtain a RH login
Place credentials in ~/.vagrant.d/Vagrantfile to enable updates for VMs by automatically registering with RedHat Subscription Manager
Vagrant.configure(&amp;#39;2&amp;#39;) do |config| config.registration.username = &amp;#39;&amp;lt;your Red Hat username&amp;gt;&amp;#39; config.registration.password = &amp;#39;&amp;lt;your Red Hat password&amp;gt;&amp;#39; end Mac OS X Prereqs Install pre-reqs:</description></item><item><title>Changing the SSL Certificate for OpenShift Console</title><link>https://dwnwrd.github.io/blog/2016/03/24/Replace-OpenShift-Console-SSL-Certificate/</link><pubDate>Thu, 24 Mar 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/03/24/Replace-OpenShift-Console-SSL-Certificate/</guid><description>OpenShift has an internal CA for generating certificates to authenticate intra-cluster communication, but your browser doesn&amp;rsquo;t trust this CA. Perhaps you want to fix that without mucking with the internal SSL communication? I did. Here is how.
This OpenShift doc explains how to do this, but it isn&amp;rsquo;t very clear, to me at least.
Overview An outline of the steps:
Only make changes to the public URLs and not any internal URLs.</description></item><item><title>OpenShift High Availability - Routing</title><link>https://dwnwrd.github.io/blog/2016/03/01/OpenShift-3-HA-Routing/</link><pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2016/03/01/OpenShift-3-HA-Routing/</guid><description>Highly availabile containers in OpenShift are baked into the cake thanks to replication controllers and service load balancing, but there are plenty of other single points of failure. Here is how to eliminate many of those.
Single Points of Failure The components of OpenShift include:
Master controller manager server and API endpoint Etcd configuration and state storage Docker Registry Router haproxy This post is mostly about adding high availability to the routing layer.</description></item><item><title>Testing OpenShift Enterprise V3</title><link>https://dwnwrd.github.io/blog/2015/07/28/Testing-OpenShift-Enterprise-V3/</link><pubDate>Tue, 28 Jul 2015 00:00:00 +0000</pubDate><guid>https://dwnwrd.github.io/blog/2015/07/28/Testing-OpenShift-Enterprise-V3/</guid><description>So much for testing OpenShift Origin with Vagrant on OS X, because it does not work yet. Let&amp;rsquo;s evaluate OpenShift Enterprise v3 on RHEL! First go get yourself an eval license. The OpenShift VMs will run RHEL7.1 and ride on top of RHEV.
Documentation First off, here are some starting points to get oriented and acquainted with OpenShift.
Docs
Getting Started Docs Overview Training Download Prerequisites OpenShift Enterprise 3 Architecture Guide - planning, deployment and operation of an Open Source Platform as a Service Load Balancing Videos</description></item></channel></rss>